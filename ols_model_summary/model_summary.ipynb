{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of statsmodels' model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import make_regression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting a regression model in statsmodels we often want to get some information about that model. The most common way of doing this is to call model.summary(). Here I'll explain in detail the meaning for the numbers returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by making some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=240, n_features=3, n_informative=2, noise=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to use the statsmodels formula API with a pandas dataframe. So I will make that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat( [pd.DataFrame(X,columns=['x0','x1','x2']) , \\\n",
    "                 pd.DataFrame(y,columns=['y'])] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.249892</td>\n",
       "      <td>0.596463</td>\n",
       "      <td>0.085399</td>\n",
       "      <td>21.100636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037913</td>\n",
       "      <td>0.803076</td>\n",
       "      <td>-1.574064</td>\n",
       "      <td>23.104803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.285393</td>\n",
       "      <td>0.339467</td>\n",
       "      <td>-1.446577</td>\n",
       "      <td>-35.216285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.963888</td>\n",
       "      <td>1.527511</td>\n",
       "      <td>0.903515</td>\n",
       "      <td>109.534170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.279441</td>\n",
       "      <td>-1.629664</td>\n",
       "      <td>-1.645524</td>\n",
       "      <td>-138.575409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2           y\n",
       "0 -2.249892  0.596463  0.085399   21.100636\n",
       "1 -0.037913  0.803076 -1.574064   23.104803\n",
       "2  0.285393  0.339467 -1.446577  -35.216285\n",
       "3 -0.963888  1.527511  0.903515  109.534170\n",
       "4 -2.279441 -1.629664 -1.645524 -138.575409"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally specifying and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = smf.ols(formula='y ~ x0+x1+x2', data=df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and printing the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.894</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.893</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   662.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 22 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>1.25e-114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:02:43</td>     <th>  Log-Likelihood:    </th> <td> -1084.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   240</td>      <th>  AIC:               </th> <td>   2177.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   236</td>      <th>  BIC:               </th> <td>   2191.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.6547</td> <td>    1.463</td> <td>    0.447</td> <td> 0.655</td> <td>   -2.228</td> <td>    3.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0</th>        <td>   -0.5257</td> <td>    1.335</td> <td>   -0.394</td> <td> 0.694</td> <td>   -3.155</td> <td>    2.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>   53.8768</td> <td>    1.339</td> <td>   40.249</td> <td> 0.000</td> <td>   51.240</td> <td>   56.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>   26.0969</td> <td>    1.503</td> <td>   17.359</td> <td> 0.000</td> <td>   23.135</td> <td>   29.059</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.168</td> <th>  Durbin-Watson:     </th> <td>   1.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.919</td> <th>  Jarque-Bera (JB):  </th> <td>   0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.062</td> <th>  Prob(JB):          </th> <td>   0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.950</td> <th>  Cond. No.          </th> <td>    1.24</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.894\n",
       "Model:                            OLS   Adj. R-squared:                  0.893\n",
       "Method:                 Least Squares   F-statistic:                     662.8\n",
       "Date:                Sun, 22 Oct 2017   Prob (F-statistic):          1.25e-114\n",
       "Time:                        11:02:43   Log-Likelihood:                -1084.5\n",
       "No. Observations:                 240   AIC:                             2177.\n",
       "Df Residuals:                     236   BIC:                             2191.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.6547      1.463      0.447      0.655      -2.228       3.538\n",
       "x0            -0.5257      1.335     -0.394      0.694      -3.155       2.104\n",
       "x1            53.8768      1.339     40.249      0.000      51.240      56.514\n",
       "x2            26.0969      1.503     17.359      0.000      23.135      29.059\n",
       "==============================================================================\n",
       "Omnibus:                        0.168   Durbin-Watson:                   1.873\n",
       "Prob(Omnibus):                  0.919   Jarque-Bera (JB):                0.181\n",
       "Skew:                           0.062   Prob(JB):                        0.913\n",
       "Kurtosis:                       2.950   Cond. No.                         1.24\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through the above one table at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"table1.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"table1.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first column here is all information you already know (or at least should know). \n",
    "\n",
    "- Df Residuals: is the degrees of freedom of residuals which calculated with N - Df model - 1 \n",
    "\n",
    "- Df model: is the degrees of freedom of the model which is just the number of predictors.\n",
    "\n",
    "- Covariance Type: Reminds us that we're doing non-robust regression. I’ll have a separate notebook introducing robust regression so I'm not going into here. only to say that ordinary least squares regression is non robust b/c it can give misleading results if its underlying assumptions are not true (I'll talk about what i mean by underlying assumptions later in this notebook).\n",
    "\n",
    "\n",
    "The second column is more interesting \n",
    "\n",
    "- R-squared (aka  coefficient of determination or Proportion of variance explained) is a statistical measure of how close the data are to the fitted regression line. you want this number to be larger than 0. if it's less than zero it means your model is useless and you would have been better off just guessing the mean value of the dependent variable. one is the largest value possible. \n",
    "\n",
    "- Adj. R-squared (adjusted R-squared) There is a problem with R-squared that is when every time you add a predictor to your model the R-squared get's closer to one (or stays the same). Because a simpler model should be prefered to a more complex one we need a metric that takes this into account. Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would occur by adding a feature with no correlation to the dependent variable and decreases otherwise.\n",
    "\n",
    "**note:** R-squared values close to one does not necessary means that you're fitting the data well. As we'll see in this notebook there is a lot that goes into producing a good model for any dataset and R-squared is just one of these numbers.\n",
    "  \n",
    "- F-statistic (aka F value) and its corresponding p-value (Prob(F)) test the overall significance of the regression model. Specifically, they help us to determine if how likely it is that all of the regression coefficients are actually equal to zero as opposed to the values shown in table two. So if Prob(F) has a value of 0.01000 then there is 1 chance in 100 that all of the regression parameters are zero. Which would indicate to us that the regression equation does have some validity in fitting the data.\n",
    "\n",
    "- Log-Likelihood: this is the value of the likelihood function with the optimal coefficients plugged in. The exact form of the Log-likelihood function it not that important but can be found here: https://en.wikipedia.org/wiki/Likelihood_function.\n",
    "\n",
    "**note:** it is never appropriate to compare Log-likelihood across models unless you have the same number of observations in each model.\n",
    "\n",
    "- BIC Bayesian information criterion (aka Schwarz criterion) and AIC Akaike information criterion are closely related metrics. Both are criterion for model selection among possible models that can be build with the features you have. The model with the lowest AIC and/or BIC is prefered. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The seccond table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"table2.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"table2.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second table is really all about the coefficients of the model.\n",
    "\n",
    "- std err (of that coefficient): is a measure of the accuracy of coef. \n",
    "\n",
    "- t (t-statistic) and P>|t| (corresponding p-value): tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis (that the real value of the coefficent is zero).\n",
    "\n",
    "- [0.025 0.975] is the 95% confidence interval of the coeffects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The third table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"table3.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"table3.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we get into finding out if our data meata the underlying assumptions of OLS regression.\n",
    "\n",
    "\n",
    "- Omnibus and Prob(Omnibus) tests whether the explained variance in a set of data is significantly greater than the unexplained variance, overall. \n",
    "\n",
    "- Skew is a measure of the skew in the dependent value;. \"Skewness\" is a measure of how symmetrical the data are; a skewed variable is one whose mean is not in the middle of the distributi\n",
    "\n",
    "- Kurtosis\" has to do with how peaked the distribution is, either too peaked or too flat. \n",
    "\n",
    "**note** \"Extreme values\" for skewness and kurtosis are values greater than +3 or less than -3.\n",
    "\n",
    "- Durbin Watson statistic is a number that tests for autocorrelation in the residuals from a statistical regression analysis. The Durbin-Watson statistic is always between 0 and 4. A value of 2 means that there is no autocorrelation in the sample.\n",
    "\n",
    "- Jarque–Bera test and Prob(JB) is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. \n",
    "\n",
    "- Cond. No. measure correlation between features. larger mean more correlation. generally any number larger than 20 is bad (in fact statsmodels will print an error if this is the case.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
